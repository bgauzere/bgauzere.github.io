<!doctype html><html lang><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><title>The “Fifth Property” of Euclidean Distances: Beyond the Metric Axioms</title>
<meta name=description content="Personal website to introduce research and teaching activities."><meta name=author content='Benoit Gaüzère'><link href="https://fonts.googleapis.com/css2?family=Inconsolata:wght@400;700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/css/bootstrap.min.css integrity=sha384-TX8t27EcRE3e/ihU7zmQxVncDAy5uIKz4rEkgIXeMed4M0jlfIDPvg6uqKI2xXr2 crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css integrity="sha512-iBBXm8fW90+nuLcSKlbmrPcLa0OT92xO1BIsZ+ywDWZCvqsWgccV3gFoRBv0z+8dLJgyAHIhR35VZc2oM/gI1w==" crossorigin=anonymous><link rel=stylesheet href=/sass/researcher.min.css><script async src="https://www.googletagmanager.com/gtag/js?id=G-BJ1N6WLD1Q"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-BJ1N6WLD1Q",{anonymize_ip:!1})}</script></head><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload=renderMathInElement(document.body)></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})})</script><body><div class="container mt-5"><nav class="navbar navbar-expand-sm flex-column flex-sm-row text-nowrap p-0"><a class="navbar-brand mx-0 mr-sm-auto" href=https://bgauzere.github.io/ title="Benoit Gaüzère website">Benoit Gaüzère website</a><div class="navbar-nav flex-row flex-wrap justify-content-center"><a class="nav-item nav-link" href=/ title=Home>Home
</a><span class="nav-item navbar-text mx-1">/</span>
<a class="nav-item nav-link" href=/news title=News>News
</a><span class="nav-item navbar-text mx-1">/</span>
<a class="nav-item nav-link" href=/posts title=Articles>Articles
</a><span class="nav-item navbar-text mx-1">/</span>
<a class="nav-item nav-link" href=/research title=Research>Research
</a><span class="nav-item navbar-text mx-1">/</span>
<a class="nav-item nav-link" href=/teaching title=Teaching>Teaching</a></div></nav></div><hr><div id=content><div class=container><p><em>This post is heavily adapted from Chapter 5 of the book <strong>Convex Optimization & Euclidean Distance Geometry</strong> by Jon Dattorro.</em></p><h2 id=metrics>Metrics</h2><p>If you work with distances, you probably already know the four classical axioms:</p><ol><li><strong>Non-negativity</strong>: $d(x, y) \ge 0$</li><li><strong>Identity of Indiscernibles</strong>: $d(x, y) = 0 \iff x = y$</li><li><strong>Symmetry</strong>: $d(x, y) = d(y, x)$</li><li><strong>Triangle Inequality</strong>: $d(x, z) \le d(x, y) + d(y, z)$</li></ol><p>But here is the catch: <strong>being a metric is not enough to be an Euclidean distance.</strong></p><p>In many machine learning applications, specifically those relying on kernel
methods like SVMs, we implicitly assume that our data points can be embedded
into a Euclidean space, and then we compute a distance, or a scalar product,
between these points to run a machine learning algorithm.</p><p>However, defining a metric between points is not sufficient to ensure that these points lie in an Euclidean space.</p><h2 id=why-a-fifth-property-at-all>Why a &ldquo;Fifth Property&rdquo; at all?</h2><p>The four metric axioms characterize <em>metrics</em>. However, <em>Euclidean</em> distances satisfy additional geometric consistency constraints: not every abstract metric can be realized as inter-point distances of some point configuration in $\mathbb{R}^d$.</p><p>Dattorro’s Euclidean distance geometry viewpoint makes this very concrete:</p><ul><li>For <strong>three points</strong>, the triangle inequality is essentially the “ultimate” realizability test.</li><li>But for <strong>four or more points</strong>, triangle inequalities are no longer sufficient to certify a Euclidean realization.</li></ul><p>This motivates a <strong>fifth Euclidean metric requirement</strong>, formulated as a <strong>relative-angle inequality</strong> that must hold at each vertex of any tetrahedron defined by four points. Intuitively, the faces of the tetrahedron must fit together in a globally consistent way.</p><p>In Dattorro’s notation, for any four distinct points ${x_i,x_j,x_\ell,x_k}$, the angles at vertex $x_k$ must satisfy constraints of the form:
$$
|\theta_{ik\ell}-\theta_{\ell kj}|\le \theta_{ikj}\le \theta_{ik\ell}+\theta_{\ell kj}
$$
If this condition is violated, the distances cannot coexist in a flat Euclidean space, even if they satisfy the triangle inequality.</p><h2 id=2-the-schoenberg-criterion-the-practical-fifth-property>2. The Schoenberg Criterion: The Practical &ldquo;Fifth Property&rdquo;</h2><p>Checking complex angle inequalities for thousands of graphs is impractical. Fortunately, there is a matrix-algebra equivalent known as the <strong>Schoenberg Criterion</strong>.</p><h3 id=the-matrix-formulation>The Matrix Formulation</h3><p>First, let’s formalize our distances. Given $N$ points ${x_1, \dots, x_N}$, we define the <strong>squared</strong> distance matrix $D \in \mathbb{R}^{N \times N}$ by:</p><p>$$
D_{ij} = ||x_i - x_j||^2
$$</p><p>Such a matrix is called a <strong>Euclidean Distance Matrix (EDM)</strong>.</p><p>Isaac Schoenberg (1935) proved that a matrix $D$ represents a valid EDM (i.e., the points actually exist in $\mathbb{R}^d$) <strong>if and only if</strong> the transformed matrix $B$:</p><p>$$
B = -\frac{1}{2} J D J, \quad \text{where } J = I - \frac{1}{N}\mathbf{1}\mathbf{1}^T
$$</p><p>is <strong>positive semidefinite</strong> (PSD). Here, $J$ is simply the geometric centering matrix.</p><h3 id=python-implementation>Python Implementation</h3><p>We can easily implement this check in Python using NumPy:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>is_euclidean_matrix</span>(D_squared):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Checks if a squared distance matrix represents a valid Euclidean embedding
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    using the Schoenberg criterion.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    N <span style=color:#f92672>=</span> D_squared<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>    <span style=color:#75715e># Geometric centering matrix</span>
</span></span><span style=display:flex><span>    J <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>eye(N) <span style=color:#f92672>-</span> np<span style=color:#f92672>.</span>ones((N, N)) <span style=color:#f92672>/</span> N
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># Compute the Gram matrix</span>
</span></span><span style=display:flex><span>    B <span style=color:#f92672>=</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>0.5</span> <span style=color:#f92672>*</span> J <span style=color:#f92672>@</span> D_squared <span style=color:#f92672>@</span> J
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># Check eigenvalues</span>
</span></span><span style=display:flex><span>    eigenvalues <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>linalg<span style=color:#f92672>.</span>eigvalsh(B)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># Allow for minor floating point errors (e.g., -1e-10 is considered 0)</span>
</span></span><span style=display:flex><span>    is_psd <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>all(eigenvalues <span style=color:#f92672>&gt;</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>1e-9</span>)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> is_psd, eigenvalues
</span></span></code></pre></div><h3 id=the-intuition-its-just-a-kernel-matrix>The Intuition: It’s just a Kernel Matrix</h3><p>Why does this obscure formula work? If you are familiar with kernel methods, this transformation should ring a bell.</p><p>We know that Euclidean distances and inner products are linked by the cosine law (or polarization identity):</p><p>$$
||x_i - x_j||^2 = ||x_i||^2 + ||x_j||^2 - 2\langle x_i, x_j \rangle
$$</p><p>The operation $B = -\frac{1}{2} J D J$ essentially inverts this relationship for the entire dataset centered at the origin. The resulting matrix $B$ is exactly the <strong>Gram matrix</strong> of the points (the matrix of inner products where $B_{ij} = \langle x_i, x_j \rangle$).</p><p>This gives us the most intuitive interpretation of the &ldquo;fifth property&rdquo;:</p><blockquote><p><strong>A distance matrix is Euclidean if and only if it can be converted into a valid Kernel matrix.</strong></p></blockquote><h3 id=what-happens-if-it-fails>What happens if it fails?</h3><p>If the condition fails (i.e., $B$ has <strong>negative eigenvalues</strong>), the &ldquo;fifth property&rdquo; is violated.</p><p>Geometrically, this implies that the implicit &ldquo;vectors&rdquo; $x_i$ would require a non-Euclidean geometry to satisfy the observed distances. In the context of Machine Learning, it means your &ldquo;kernel&rdquo; is not valid, potentially breaking algorithms like SVMs that rely on the PSD property. The kernel deduced from a non Euclidean Distance Matrix is thus not valid.</p><h2 id=3-illustration-the-impossible-tetrahedron>3. Illustration: The &ldquo;Impossible&rdquo; Tetrahedron</h2><p>Let&rsquo;s verify this behavior with a concrete example.</p><h3 id=step-1-three-points-always-euclidean>Step 1: Three points (Always Euclidean)</h3><p>Let&rsquo;s define 3 points with distances corresponding to a right-angled triangle: sides 1, 2, and hypotenuse.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>D3 <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array([
</span></span><span style=display:flex><span>    [<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>5</span>],
</span></span><span style=display:flex><span>    [<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>4</span>],
</span></span><span style=display:flex><span>    [<span style=color:#ae81ff>5</span>, <span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>])
</span></span><span style=display:flex><span>valid, eigs <span style=color:#f92672>=</span> is_euclidean_matrix(D3)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Triangle valid? </span><span style=color:#e6db74>{</span>valid<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>) 
</span></span><span style=display:flex><span><span style=color:#75715e># Output: Triangle valid? True</span>
</span></span></code></pre></div><p>For three points, triangle inequality is sufficient. So this triangle, as Pythagore said, is valid.</p><h3 id=step-2-adding-the-4th-point-the-metric-trap>Step 2: Adding the 4th point (The Metric Trap)</h3><p>We add a 4th point. We fix most distances, but leave $d_{14}$ variable.
We have 4 points with fixed distances:</p><ul><li>$d_{12}=1, d_{24}=1, d_{43}=1$</li><li>$d_{13}=\sqrt{5}$</li></ul><figure><img src=polyhedron.svg alt="Four-point example showing that metric axioms do not guarantee Euclidean embedding. While the interval $[\sqrt{5}-1, 2]$ is valid for a metric, only specific values form a valid Euclidean shape."><figcaption><h4>The 'Impossible' Tetrahedron</h4><p>Four-point example showing that metric axioms do not guarantee Euclidean embedding. While the interval $[\sqrt{5}-1, 2]$ is valid for a metric, only specific values form a valid Euclidean shape.</p></figcaption></figure><p>We want to find the missing distance $d_{14}$.</p><h3 id=the-metric-interval-vs-the-euclidean-point>The Metric Interval vs. The Euclidean Point</h3><p>The triangle inequality constrains $d_{14}$ to lie in the interval $[\sqrt{5}-1, 2]$. Any value in this range makes $d$ a valid metric.</p><p>However, Euclidean geometry is rigid. The Schoenberg criterion will show that
only <strong>one</strong> specific value in this interval allows for a Euclidean embedding.</p><p>Let&rsquo;s pick a value inside this interval, say <strong>1.8</strong>. This creates a valid metric space.
Squared, this gives $1.8^2 = 3.24$.</p><p>Let&rsquo;s check if this is Euclidean using our Python function:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Distances for the 4-point example</span>
</span></span><span style=display:flex><span><span style=color:#75715e># We choose d(1,4) = 1.8, which satisfies triangle inequality</span>
</span></span><span style=display:flex><span>d14_squared <span style=color:#f92672>=</span> <span style=color:#ae81ff>1.8</span><span style=color:#f92672>**</span><span style=color:#ae81ff>2</span> 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>D4 <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array([
</span></span><span style=display:flex><span>    [<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>5</span>, d14_squared],
</span></span><span style=display:flex><span>    [<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>1</span>],
</span></span><span style=display:flex><span>    [<span style=color:#ae81ff>5</span>, <span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>],
</span></span><span style=display:flex><span>    [d14_squared, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>valid, eigs <span style=color:#f92672>=</span> is_euclidean_matrix(D4)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Tetrahedron valid? </span><span style=color:#e6db74>{</span>valid<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Eigenvalues: </span><span style=color:#e6db74>{</span>np<span style=color:#f92672>.</span>round(eigs, <span style=color:#ae81ff>2</span>)<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span></code></pre></div><p><strong>Output:</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>Tetrahedron valid? False
</span></span><span style=display:flex><span>Eigenvalues: [-0.15 -0.    0.8   3.16]
</span></span></code></pre></div><p>The negative eigenvalue ($\simeq -0.15$) proves that this configuration is <strong>impossible</strong> in Euclidean space, despite satisfying all metric axioms.</p><h2 id=4-visualizing-the-fifth-property>4. Visualizing the &ldquo;Fifth Property&rdquo;</h2><p>To visualize exactly how restrictive the Euclidean condition is compared to the metric condition, we can plot the smallest eigenvalue of $B$ as a function of the distance $d_{14}$.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> matplotlib.pyplot <span style=color:#66d9ef>as</span> plt
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>d_vals <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>linspace(<span style=color:#ae81ff>1.2</span>, <span style=color:#ae81ff>2.1</span>, <span style=color:#ae81ff>100</span>)
</span></span><span style=display:flex><span>min_eigs <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> d <span style=color:#f92672>in</span> d_vals:
</span></span><span style=display:flex><span>    <span style=color:#75715e># Reconstruct matrix for each d</span>
</span></span><span style=display:flex><span>    D_temp <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array([
</span></span><span style=display:flex><span>        [<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>5</span>, d<span style=color:#f92672>**</span><span style=color:#ae81ff>2</span>],
</span></span><span style=display:flex><span>        [<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>1</span>],
</span></span><span style=display:flex><span>        [<span style=color:#ae81ff>5</span>, <span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>],
</span></span><span style=display:flex><span>        [d<span style=color:#f92672>**</span><span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>    ])
</span></span><span style=display:flex><span>    _, eigs <span style=color:#f92672>=</span> is_euclidean_matrix(D_temp)
</span></span><span style=display:flex><span>    min_eigs<span style=color:#f92672>.</span>append(np<span style=color:#f92672>.</span>min(eigs))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Plotting</span>
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>figure(figsize<span style=color:#f92672>=</span>(<span style=color:#ae81ff>10</span>, <span style=color:#ae81ff>5</span>))
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>plot(d_vals, min_eigs, label<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;Smallest Eigenvalue of Gram Matrix&#39;</span>, linewidth<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>axhline(<span style=color:#ae81ff>0</span>, color<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;black&#39;</span>, linestyle<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;--&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>axvline(np<span style=color:#f92672>.</span>sqrt(<span style=color:#ae81ff>2</span>), color<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;red&#39;</span>, linestyle<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;--&#39;</span>, label<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;Euclidean Solution ($\sqrt</span><span style=color:#e6db74>{2}</span><span style=color:#e6db74>$)&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Highlight the valid metric interval (Green zone)</span>
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>axvspan(np<span style=color:#f92672>.</span>sqrt(<span style=color:#ae81ff>5</span>)<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>, color<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;green&#39;</span>, alpha<span style=color:#f92672>=</span><span style=color:#ae81ff>0.1</span>, label<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;Valid Metric Interval&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>title(<span style=color:#e6db74>&#39;Metric Validity vs. Euclidean Validity&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>xlabel(<span style=color:#e6db74>&#39;Distance $d_</span><span style=color:#e6db74>{14}</span><span style=color:#e6db74>$&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>legend()
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>grid(<span style=color:#66d9ef>True</span>, alpha<span style=color:#f92672>=</span><span style=color:#ae81ff>0.3</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>savefig(<span style=color:#e6db74>&#34;fifth_property.svg&#34;</span>)
</span></span></code></pre></div><figure><img src=fifth_property.svg alt="The green zone shows where the triangle inequality holds. However, the matrix is only Euclidean where the curve is non-negative. This happens at exactly one point: $d_{14} = \sqrt{2}$."><figcaption><h4>The gap between Metric and Euclidean</h4><p>The green zone shows where the triangle inequality holds. However, the matrix is only Euclidean where the curve is non-negative. This happens at exactly one point: $d_{14} = \sqrt{2}$.</p></figcaption></figure><h2 id=an-so-what->An so what ?</h2><p>Not all distances imply that a corresponding embedding in Euclidean space exists. This has consequences on the way you define kernels from your distances. If your distance matrix is not an EDM, your kernel will not be positive semidefinite (PSD).</p><p>Considering graphs, the Graph Edit Distance (GED) generally fails to satisfy these additional Euclidean properties. Consequently, the pairwise GED matrix is not guaranteed to correspond to squared Euclidean distances. As a consequence, kernels derived from GED values should be regularized, which induces bias in the kernel matrix.</p><p><em>Note: Methods using the GED for empirical maps are not directly affected by this point, as they treat distances as features rather than inner products.</em></p><p><strong>Takeaway</strong>: GED should be interpreted as a generic dissimilarity measure. If you feed GED into a kernel machine (SVM, Gaussian Process), be aware that your kernel matrix might not be PSD. You may need to apply corrections—such as spectrum clipping (setting negative eigenvalues to zero)—to enforce the &ldquo;fifth property&rdquo; artificially.</p><hr><p><a href=5thproperty.py>Download the full Python script</a></p><hr><h2 id=references>References</h2><ul><li><strong>Dattorro, J.</strong> (2011). <em>Convex Optimization & Euclidean Distance Geometry</em>. Meboo Publishing. (Chapter 5: Euclidean Distance Matrix).</li><li><strong>Riesen, K., & Bunke, H.</strong> (2010). <em>Graph Classification and Clustering Based on Vector Space Embedding</em>. World Scientific.</li><li><strong>Schölkopf, B., & Smola, A. J.</strong> (2002). <em>Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond</em>. MIT Press.</li></ul><hr><p><em>Disclaimer: The synthesis and drafting of this post were assisted by AI, while the information collection and curation were performed and are assumed by me. If a factual error is present or a relevant resource is missing, please notify me by email.</em></p></div></div><div id=footer class=mb-5><hr><div class="container text-center"><a href=https://twitter.com/BGauzere class="fab fa-twitter fa-1x" title=Twitter></a><a href=mailto:benoit.gauzere@insa-rouen.fr class="fas fa-envelope fa-1x" title=E-mail></a><a href=https://github.com/bgauzere/ class="fab fa-github fa-1x" title=Github></a><a href="https://scholar.google.com/citations?hl=en&amp;user=YqmqE9gAAAAJ" class="fab fa-google fa-1x" title="Google scholar"></a></div><div class="container text-center"><a href=https://bgauzere.github.io/ title="Benoit Gaüzère 2022"><small>Benoit Gaüzère 2022</small></a></div></div></body></html>